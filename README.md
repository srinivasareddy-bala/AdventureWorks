An end to end data engineering project on adventure works dataset right from ingestion of data from diverse sources to exposing the data through PowerBI. Below are the brief steps about what I did.
Ingested data from different sources such as database tables, api end points, github to Azure Data Lake using Azure Data Factory.
Did transformations by loading the data using pyspark in Azure Databricks and store them as parquet files in the Azure Data Lake. 
Loading the data from Azure Data Lake into Azure synapse analytics to perform analytics by creating external source, file format and tables leveraging the power of azure synapse. 
Finally, exposed the data through the PowerBI visualization tool.
